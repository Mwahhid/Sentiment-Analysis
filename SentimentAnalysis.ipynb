{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef33273f-14e8-4ff2-a79a-7f8de44afb3f",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "In this project, I am creating two models for binary classification of text sentiments. I used Python's TensorFlow library to create a model using Recurrent Neural Network (RNN) as my first model while for the second model I used the XGBoost library to build decision trees. I will be training both of these models on movie reviews to see how well they can predict whether a review is positive or negative. I will also be training them on Steam reviews to be able to predict whether a person liked the game or not from their use of words on Steam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9d3a0fb-005a-4005-9308-822dbb7a9521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca02e081-3d99-40dc-84dc-1b51ea19b165",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the dataset\n",
    "# To run the model on tweets, use 'Steam_reviews.csv', while for movie reviews use 'IMDB Dataset.csv'\n",
    "\n",
    "fileName = 'IMDB Dataset.csv'\n",
    "data = pd.read_csv(fileName)\n",
    "print(len(data))\n",
    "data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c04fe74-3fdb-4e94-9430-c1dc44a1f80b",
   "metadata": {},
   "source": [
    "### Pre-processing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75defebc-8964-4e07-82a8-857121799f47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[one, of, the, other, reviewers, has, mentione...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production the filming tech...</td>\n",
       "      <td>1</td>\n",
       "      <td>[a, wonderful, little, production, the, filmin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "      <td>[i, thought, this, was, a, wonderful, way, to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically theres a family where a little boy j...</td>\n",
       "      <td>0</td>\n",
       "      <td>[basically, theres, a, family, where, a, littl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love in the time of money is a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[petter, matteis, love, in, the, time, of, mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>i thought this movie did a down right good job...</td>\n",
       "      <td>1</td>\n",
       "      <td>[i, thought, this, movie, did, a, down, right,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>bad plot bad dialogue bad acting idiotic direc...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bad, plot, bad, dialogue, bad, acting, idioti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>i am a catholic taught in parochial elementary...</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, am, a, catholic, taught, in, parochial, el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>im going to have to disagree with the previous...</td>\n",
       "      <td>0</td>\n",
       "      <td>[im, going, to, have, to, disagree, with, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>no one expects the star trek movies to be high...</td>\n",
       "      <td>0</td>\n",
       "      <td>[no, one, expects, the, star, trek, movies, to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment  \\\n",
       "0      one of the other reviewers has mentioned that ...          1   \n",
       "1      a wonderful little production the filming tech...          1   \n",
       "2      i thought this was a wonderful way to spend ti...          1   \n",
       "3      basically theres a family where a little boy j...          0   \n",
       "4      petter matteis love in the time of money is a ...          1   \n",
       "...                                                  ...        ...   \n",
       "49995  i thought this movie did a down right good job...          1   \n",
       "49996  bad plot bad dialogue bad acting idiotic direc...          0   \n",
       "49997  i am a catholic taught in parochial elementary...          0   \n",
       "49998  im going to have to disagree with the previous...          0   \n",
       "49999  no one expects the star trek movies to be high...          0   \n",
       "\n",
       "                                                  tokens  \n",
       "0      [one, of, the, other, reviewers, has, mentione...  \n",
       "1      [a, wonderful, little, production, the, filmin...  \n",
       "2      [i, thought, this, was, a, wonderful, way, to,...  \n",
       "3      [basically, theres, a, family, where, a, littl...  \n",
       "4      [petter, matteis, love, in, the, time, of, mon...  \n",
       "...                                                  ...  \n",
       "49995  [i, thought, this, movie, did, a, down, right,...  \n",
       "49996  [bad, plot, bad, dialogue, bad, acting, idioti...  \n",
       "49997  [i, am, a, catholic, taught, in, parochial, el...  \n",
       "49998  [im, going, to, have, to, disagree, with, the,...  \n",
       "49999  [no, one, expects, the, star, trek, movies, to...  \n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprocessing the data:\n",
    "\n",
    "# For movie reviews, uncomment this block of code and comment out the other one:\n",
    "\n",
    "data['review'] = data['review'].str.replace('<br /><br />', '')\n",
    "data['review'] = data['review'].str.replace('[^\\w\\s]','', regex=True).str.lower()\n",
    "data['tokens'] = data['review'].str.split() #tokenizing to see if there are unnecessary characters\n",
    "data['sentiment'] = (data['sentiment'] == 'positive').astype(int)\n",
    "\n",
    "\n",
    "# For steam reviews, uncomment the following block of code and comment out the other one:\n",
    "\n",
    "# data['review'] = data['user_review'].str.replace('[^\\w\\s]','', regex=True).str.lower()\n",
    "# data['sentiment'] = data['user_suggestion']\n",
    "\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce8206ac-c7dd-4962-a333-bf1fa5f29f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra columns for steam reviews dataset only. Uncomment the following lines for steam reviews dataset but comment for movie reviews.\n",
    "# data = data[['review', 'sentiment']]\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b93eee58-b1e1-4244-9650-dad416618a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000,)\n",
      "(40000,)\n",
      "(10000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data into training and testing set using scikit-learn's split\n",
    "#funtion\n",
    "\n",
    "# For movie reviews, uncomment this block of code and comment out the other one:\n",
    "\n",
    "X_train = data['review'][:40000]\n",
    "y_train = data['sentiment'][:40000]\n",
    "X_test = data['review'][40000:]\n",
    "y_test = data['sentiment'][40000:]\n",
    "\n",
    "# For steam reviews, uncomment the following block of code and comment out the other one:\n",
    "\n",
    "# X_train = data['review'][:10000]\n",
    "# y_train = data['sentiment'][:10000]\n",
    "# X_test = data['review'][10000:]\n",
    "# y_test = data['sentiment'][10000:]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d6a934-9ddc-4ff1-aa65-4a24050467a6",
   "metadata": {},
   "source": [
    "## Model 1 : Forward Propogation Neural Network using TensorFlow and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dd72af1-d299-4035-9162-c0573429cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a tokenizer using Keras from TensorFlow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# Only keep 10,000 most common words and call other words 'out of \n",
    "# vocabulary words' or 'OOV'\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "#Building tokenizer vocabulary on the training data\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "#print(dict(list(tokenizer.word_index.items())[0:30])) #uncomment to see the 30 most common tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57743e37-f609-4f1e-bac7-b3fbec4ed903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the text into numerical sequence\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6e9659b-023a-4ad4-bf50-447d99e3fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding the sequences to ensure all sequences have the same length because\n",
    "# we need to input all sequences of same length into the NN to train it.\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#limiting the length to 100 tokens\n",
    "max_length = 100\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6ef909c-59b9-4068-a066-d789105ebb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 16)           160000    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                272       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Defining the model: FeedForward NN using the sigmoid activation function.\n",
    "# the first layer is an embedding layer, the last layer is a single neuron with\n",
    "# sigmoid activation function which produces a probability between 0 and 1.\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=10000, output_dim=16, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7341c8f-6937-429f-ab5f-3f144f7e807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model while using binary cross entropy loss\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e66b81b-862a-4d77-908d-1a1ec375ecbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.4836 - accuracy: 0.7788 - val_loss: 0.3716 - val_accuracy: 0.8344\n",
      "Epoch 2/5\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.3229 - accuracy: 0.8602 - val_loss: 0.3577 - val_accuracy: 0.8427\n",
      "Epoch 3/5\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.2859 - accuracy: 0.8796 - val_loss: 0.3645 - val_accuracy: 0.8430\n",
      "Epoch 4/5\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.2644 - accuracy: 0.8905 - val_loss: 0.3800 - val_accuracy: 0.8367\n",
      "Epoch 5/5\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.2479 - accuracy: 0.8984 - val_loss: 0.3944 - val_accuracy: 0.8360\n"
     ]
    }
   ],
   "source": [
    "#Training and Evaluating the model\n",
    "\n",
    "#Running this again will continue running more iterations which will lead to overfitting.\n",
    "history = model.fit(X_train_pad, y_train, epochs=5, validation_data=(X_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7707474c-8158-4bd1-aa13-3a78d897422a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 872us/step - loss: 0.2134 - accuracy: 0.9187\n",
      "Training Accuracy:  0.9186750054359436\n",
      "Training Loss:  0.2134408950805664\n",
      "313/313 [==============================] - 0s 751us/step - loss: 0.3944 - accuracy: 0.8360\n",
      "Testing Accuracy:  0.8360000252723694\n",
      "Testing Loss:  0.39441898465156555\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_accuracy = model.evaluate(X_train_pad, y_train)\n",
    "print(\"Training Accuracy: \", train_accuracy)\n",
    "print(\"Training Loss: \", train_loss)\n",
    "test_loss, test_accuracy = model.evaluate(X_test_pad, y_test)\n",
    "print(\"Testing Accuracy: \", test_accuracy)\n",
    "print(\"Testing Loss: \", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f47c15f-c385-4d6b-8324-96662b6dcf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a movie review:  Absolute sh*te game! DO NOT PLAY!!!  Lets be honest here, its a game with brilliant graphics, but that's the only positive about this game. This game throws you in at the deep end with other players who have been playing this game for a very long time and will NOT give you a chance to fire a shot. Even if you did manage to get a shot off, it will no doubt do zero damage to the target while apparently your own armour might as well be made out of hopes and dreams!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 75ms/step\n",
      "Negative review\n"
     ]
    }
   ],
   "source": [
    "#Predicting a review from the user.\n",
    "if fileName == 'Steam_reviews.csv':\n",
    "    review = input('Enter a game review: ')\n",
    "else:\n",
    "    review = input('Enter a movie review: ')\n",
    "review = tokenizer.texts_to_sequences([review])\n",
    "review = pad_sequences(review, maxlen=100)\n",
    "prediction = model.predict(review)\n",
    "\n",
    "# Print the predicted sentiment\n",
    "if prediction > 0.5:\n",
    "    print('Positive review')\n",
    "else:\n",
    "    print('Negative review')\n",
    "    \n",
    "#It does better on longer reviews than short reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae91257-3f98-405c-9592-ba6d2dc90388",
   "metadata": {},
   "source": [
    "## Model 2 : Decision Trees using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c1b6be6-d085-4a85-81c0-dd84303ddec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all the cells in the preprocessing section before running this model. \n",
    "# (Don't run this cell consecutively or you will need to preprocess again!)\n",
    "\n",
    "#Now we extract features using Scikit-learn's CountVectorizer:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer() # We cannot use Keras's tokenizer for XGBoost because we need tokens to be vectorized\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c212ff71-e5bd-459f-b917-0e15da5c8f30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining and training the decision tree model with binary classifier\n",
    "\n",
    "model2 = xgb.XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=100, objective='binary:logistic')\n",
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72f348eb-b84c-41e3-a1e5-8c472de63f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8532\n",
      "Testing Accuracy: 0.8264\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the model using Scikit-learn's model evaluation functions:\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "pred1 = model2.predict(X_train)\n",
    "pred2 = model2.predict(X_test)\n",
    "\n",
    "print('Training Accuracy:', accuracy_score(y_train, pred1))\n",
    "print('Testing Accuracy:', accuracy_score(y_test, pred2))\n",
    "#print('Confusion matrix:', confusion_matrix(y_test, predictions)) #Uncomment to see the confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0a36f3-0ddb-4f6c-b9ad-3dce882cae5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
